{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5k2jxq5Cxwum",
        "outputId": "2acd8ebe-d129-40b2-8f48-9a692c2e378a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nPLEASE FOLLOW INSTRUCTIONS ON THIS CELL BEFORE RUNNING THIS NOTEBOOK.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "PLEASE FOLLOW INSTRUCTIONS ON THIS CELL BEFORE RUNNING THIS NOTEBOOK.\n",
        "\"\"\"\n",
        "\n",
        "# In order for this file to work, this cell needs to be run first\n",
        "# Remove the '#' on the last two lines and run the cell\n",
        "# When it is done the Runtime needs to be restarted at Runtime -> Restart Runtime\n",
        "# Please make the last two lines adding '#' at the beginning.\n",
        "# Please wait until ALL commands have run\n",
        "\n",
        "#!pip install spacy --upgrade\n",
        "#!python -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7PWwNqElDIsz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import spacy\n",
        "import en_core_web_md\n",
        "from io import open\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yASoRrXUAc-p"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq8_myMpAe-Q"
      },
      "source": [
        "This is one of three supporting notebooks that supports this thesis.\n",
        "\n",
        "In this notebook has the code for all the sections in the methodology section before machine learning model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TroDbvwnA_WC"
      },
      "source": [
        "# 5.1 Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywtPpLqOCg46"
      },
      "source": [
        "## 5.1.1 Text Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_JhprqvDZRY",
        "outputId": "b2473f36-5003-4cb4-91aa-4b662a19b222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'central_bank_thesis'...\n",
            "remote: Enumerating objects: 311, done.\u001b[K\n",
            "remote: Counting objects: 100% (204/204), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 311 (delta 15), reused 135 (delta 6), pack-reused 107\u001b[K\n",
            "Receiving objects: 100% (311/311), 17.08 MiB | 12.40 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "# Get files from github repository\n",
        "!git clone https://github.com/luissejas/central_bank_thesis.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZdVAVoHFPQtj"
      },
      "outputs": [],
      "source": [
        "path = r'/content/central_bank_thesis/text_files'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU6XvA0ACu-x"
      },
      "source": [
        "## 5.1.2 Labeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t4Oi0XHkHzF7"
      },
      "outputs": [],
      "source": [
        "# Creating the lists of the paths of both categories\n",
        "adversity_files = []\n",
        "prosperity_files = []\n",
        "\n",
        "adversity_years = ['2006', '2007', '2008', '2009', '2010', '2020', '2021', '2022']\n",
        "\n",
        "\n",
        "# Looping through the files to categorize them\n",
        "for file in os.listdir(path):\n",
        "    if file[0:4] in adversity_years: # file [0:4] are the character positions of the year\n",
        "        adversity_files.append(os.path.join(path, file))\n",
        "    else:\n",
        "        prosperity_files.append(os.path.join(path, file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxppkWRBDSKE"
      },
      "source": [
        "# 5.2 Vocabulary Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS8_WnjXEtN6"
      },
      "source": [
        "Now that we have a list containing the full path the files,we will iterate through them and 'read' the file with spacy.\n",
        "\n",
        "Spacy will read the file and annotate the text with part of speech tags (POS).\n",
        "\n",
        "A complete list of POS tags can be found in the following link, https://universaldependencies.org/u/pos/\n",
        "\n",
        "In this case, I do not want the model to be bound by context, so I will focus on four part-of-speech tags: adjective, adverb, noun and verb.\n",
        "\n",
        "I also want to remove stopwords. Stopwords are words that do not add any predictive power to the text.\n",
        "\n",
        "Lastly, I want to lemmatize each word. Lemmatization means to get the core meaning of each word, this reduces vocabulary size and provides more predictive power to the NLP model. For example \"made\" is a form from the verb \"make\", therefore with lemmatization \"made\" -> \"make\". Instead of using two words that have the same core meaning, it is more efficient to keep one reference with the same core meaning.\n",
        "\n",
        "The code below will do the following:\n",
        "\n",
        "1) Open the file\n",
        "\n",
        "2) Let SpaCy read the file (also considered a document [doc])\n",
        "\n",
        "3) Go through each word (token) in the doc and determine that it passes the filter: the required part of speech, not a stopword and not a repeated lemma of the word.\n",
        "\n",
        "4) If the word passes all filters, it is added to the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzNJJOrKFqZH"
      },
      "source": [
        "## 5.2.1 Lemmatization and Part-of-Speech Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gwcLdR1iFAr_"
      },
      "outputs": [],
      "source": [
        "# SpaCy requires the language pipeline to be loaded\n",
        "nlp = en_core_web_md.load()\n",
        "\n",
        "# Stopwords from spacy\n",
        "stopwords = list(nlp.Defaults.stop_words)\n",
        "\n",
        "# I also want to filter the desired POS tag\n",
        "pos_list = ['ADJ', 'ADV', 'VERB', 'NOUN']\n",
        "\n",
        "# Create an empty instance of the vocabulary \n",
        "vocabulary = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l6TVVZlJPCN_"
      },
      "outputs": [],
      "source": [
        "file_path_list = []\n",
        "\n",
        "# Looping through files in directory and keeping their full paths on a list.\n",
        "# This helps you work with all of the files that are in the same place with different code\n",
        "for file in os.listdir(path):\n",
        "    file_path_list.append(os.path.join(path, file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EXO_oYaEFHmj"
      },
      "outputs": [],
      "source": [
        "# Loop through files and create vocabulary\n",
        "\n",
        "for file in file_path_list:\n",
        "\n",
        "    file_content = open(file, mode='r', encoding='utf-8').read() # reading the file to get the text\n",
        "    file_content_doc = nlp(file_content) # applying spacy to read the file\n",
        "\n",
        "    # Now we want to go through each token (word) in the document, get its lemma and if it passes all filters,\n",
        "    # add it to our vocabulary\n",
        "\n",
        "    for token in file_content_doc:\n",
        "        if token.lemma_ in stopwords: # If it is a stopword, skip and proceed to the next word\n",
        "            pass\n",
        "        else:\n",
        "            if token.pos_ in pos_list: # Checking if the part-of-speech tag is one of the four listed\n",
        "                if token.lemma_ in vocabulary: # If it is approved, I check if the word is already on the vocabulary\n",
        "                    pass                      # If the word already is on the vocabulary, skip to the next.\n",
        "                else:\n",
        "                    vocabulary.append(token.lemma_) #If not, add it to the vocabulary\n",
        "            else:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uWP_IuP5GMB-"
      },
      "outputs": [],
      "source": [
        "# Once we have completed our list, we export it to a pandas dataframe\n",
        "vocabulary_df = pd.DataFrame({'word':vocabulary})\n",
        "\n",
        "# Export it to csv\n",
        "# The file is deliberately named this way because the vocabulary on the next section is the filtered one.\n",
        "vocabulary_df.to_csv('vocabulary_v1.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PbbEmnxGDWB"
      },
      "source": [
        "## 5.2.2 Vocabulary Refinement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7nEb-2zvGHXy"
      },
      "outputs": [],
      "source": [
        "# For this code to work, the vocabulary needs to be saved on the same folder as this notebook\n",
        "# Otherwise, consider putting the full path of the csv file\n",
        "revised_vocabulary = pd.read_csv('/content/central_bank_thesis/methodology_before_training/vocabulary.csv')\n",
        "\n",
        "accepted_words = revised_vocabulary['word'].values.tolist()\n",
        "\n",
        "excluded_words = []\n",
        "\n",
        "# We have to loop through all files again, with the added filter of accepted and excluded_words, allowing more transparency during\n",
        "# the process\n",
        "\n",
        "for file in file_path_list:\n",
        "\n",
        "    file_content = open(file, mode='r', encoding='utf-8').read() # reading the file to get the text\n",
        "    file_content_doc = nlp(file_content) # applying spacy to read the file\n",
        "\n",
        "    for token in file_content_doc:\n",
        "        if token.lemma_ in stopwords: # If it is a stopword, skip and proceed to the next word\n",
        "            pass\n",
        "        else:\n",
        "            if token.pos_ in pos_list: # Checking if the part-of-speech tag is one of the four listed\n",
        "                if token.lemma_ in accepted_words or token.lemma_ in excluded_words: # Checking if the word is already on a list\n",
        "                    pass\n",
        "                else:\n",
        "                    excluded_words.append(token.lemma_) #If not in vocabulary, add list to excluded words\n",
        "            else:\n",
        "                pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Zkmrm2ggGzSy"
      },
      "outputs": [],
      "source": [
        "# Exporting the excluded words for comparison with the accepted words\n",
        "\n",
        "excluded_words_df = pd.DataFrame({'excluded_words':excluded_words})\n",
        "excluded_words_df.to_csv('excluded_words.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOCCsJAqHG7q"
      },
      "source": [
        "# 5.3 Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvsU0ShBJ3yb"
      },
      "source": [
        "## 5.3.1 Text Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Crc0Ee6HdBk"
      },
      "source": [
        "Text transformation needs to be carried out twice: once for the adversity files.\n",
        "\n",
        " It comprises five parts:\n",
        "\n",
        "- 1) Sentence Filter\n",
        "\n",
        "- 2) Dataframe Creator\n",
        "\n",
        "- 3) Dataframe Cleaning\n",
        "\n",
        "- 4) Text Block Creation\n",
        "\n",
        "- 5) Labeled Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgt3zgUTKLpL"
      },
      "source": [
        "### 5.3.1.1 Sentence Filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apD23OBrKae_"
      },
      "source": [
        "Sentence Filter extracts approved words from each sentence and returns a filtered sentence. Every word in the document will go through if statements that will check if the word's lemma is in the vocabulary and if the word has the desired part-of-speech tag. Then, the function will return the sentence with the approved words.\n",
        "\n",
        "This function will only be called inside the next function (5.3.1.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aCkmY6uUHJcA"
      },
      "outputs": [],
      "source": [
        "def filter_fed_sentence (vocabulary: list, pos_list: list, sentence):\n",
        "    \"\"\"\n",
        "    This function will analyze every sentence in a document as it is done in the create_dataframe process.\n",
        "\n",
        "    The 'filtered_sentence' variable is a list made to store the approved words in a given sentence.\n",
        "\n",
        "    If there are approved words in a sentence, they will be returned as string separated by space.\n",
        "\n",
        "    If no words are found only the string '---filler--' will be returned. This is a key word that will be used to filter the\n",
        "    sentences that are no longer useful.\n",
        "    \"\"\"\n",
        "    filtered_sentence = []\n",
        "\n",
        "    for token in sentence:\n",
        "        if token.lemma_ in vocabulary and token.pos_ in pos_list:\n",
        "            filtered_sentence.append(token.lemma_)\n",
        "\n",
        "\n",
        "    if len(filtered_sentence) == 0:\n",
        "        filtered_sentence.append(\"---filler---\")\n",
        "        return \"\".join(filtered_sentence)\n",
        "    else:\n",
        "        return \" \".join(filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEF3gzkiK5kq"
      },
      "source": [
        "### 5.3.1.2 Dataframe Creator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSNl3F-0LEYb"
      },
      "source": [
        "The Dataframe creator creates a pandas dataframe that returns the original sentence in one column to the extracted sentence in another column. This means that the original sentence will be on the same row as the extracted sentence. This process allows me to examine the results of my approved vocabulary, giving me the flexibility to adjust the vocabulary if I spot a word I do not consider relevant to the machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z2gjNYaILBSp"
      },
      "outputs": [],
      "source": [
        "def create_dataframe (filepath: list, vocabulary: list, pos_list: list, isAdversity: bool):\n",
        "    \"\"\"\n",
        "    For this function, a dictionary will be used to create the dataset and at the end converted to a pandas dataframe.\n",
        "\n",
        "    In every step before converting to a pandas dataframe, all steps will be stored in the dictionary\n",
        "\n",
        "    Functions will do the following steps:\n",
        "\n",
        "    1) Loop through each file in the file list\n",
        "\n",
        "        1.1) Open a file\n",
        "\n",
        "        1.2) Process its contents with SpaCy\n",
        "\n",
        "            1.2.1) Loop through each sentence (as separated by SpaCy)\n",
        "\n",
        "            1.2.2) Collect the original sentence\n",
        "\n",
        "            1.2.3) Store the sentence into a column\n",
        "\n",
        "            1.2.4) Filter the sentence as done by the function 'filter_fed_sentence'\n",
        "\n",
        "            1.2.5) Store the result of a new function on another column but the same row\n",
        "        \n",
        "    2) Save the processed content into a DataFrame\n",
        "\n",
        "    3) If the file is from the adversity years, export its content as .csv. The same is applied on the case of growth years\n",
        "\n",
        "    4) Return the generated pandas dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_dict = {\n",
        "    'original_sentence':['---filler---'],\n",
        "    'filtered_sentence':['---filler---']\n",
        "    }\n",
        "\n",
        "    for file in filepath:\n",
        "        file_content = open(file, mode='r', encoding='utf-8').read()\n",
        "        file_content_doc = nlp(file_content)\n",
        "\n",
        "        for sentence in file_content_doc.sents:\n",
        "            dataset_dict['original_sentence'].append(sentence.text)\n",
        "            dataset_dict['filtered_sentence'].append(filter_fed_sentence(vocabulary=vocabulary, pos_list=pos_list, sentence=sentence))\n",
        "    \n",
        "    dataset_df = pd.DataFrame(dataset_dict)\n",
        "\n",
        "    if isAdversity:\n",
        "        dataset_df.to_csv('adversity_sentences.csv')\n",
        "    else:\n",
        "        dataset_df.to_csv('prosperity_sentences.csv')\n",
        "    \n",
        "    return dataset_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfVtyDJLLXaT"
      },
      "source": [
        "### 5.3.1.3 Dataframe Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpM9PzvALcwc"
      },
      "source": [
        "The Dataframe Cleaning cleans the dataframe to only keep the filtered sentences. SpaCy processes the text from its pre-trained model and both the FOMC meeting and press conference transcripts do not have an ordinary text structure. This means there will be a number of sentences with non-relevant words and they need to be taken out of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pPG3IZhYLPcF"
      },
      "outputs": [],
      "source": [
        "def clean_dataframe (df) -> list:\n",
        "    \"\"\"\n",
        "    This function is to be used only after the contents of the sentences are considered satisfactory\n",
        "\n",
        "    It will take as input a pandas dataframe (df variable) remove the original sentence and it will return a list of the values\n",
        "    of the filtered sentences.\n",
        "    \"\"\"\n",
        "    remove_column = df.drop(columns=['original_sentence'])\n",
        "\n",
        "    final_df = remove_column[remove_column['filtered_sentence'] != '---filler---']\n",
        "\n",
        "    sentence_list = final_df['filtered_sentence'].values.tolist()\n",
        "\n",
        "    return sentence_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s1dP3hPLrY8"
      },
      "source": [
        "### 5.3.1.4 Text Block Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSXWg1WxLrPj"
      },
      "source": [
        "Text Block Creation aims to avoid a problem that is specific to the Federal Reserve's case. Regardless whether the United States economy will be in a period of prosperity or adversity, words such as inflation and interest rates will be mentioned. The word inflation by itself does not provide information of the Federal Reserve's future actions or perspectives regarding inflation. Terms such as \"high inflation\" or \"rising inflation\" are more informative than the word inflation. Given the size of the data collected, it is not feasible to manually revise every filtered sentence. With some data exploration, I found out that the longest filtered sentence consisted of 72 words . I considered text length of 72 words to be long enough to provide the machine learning model with predictive power and allow the model to analyze detailed FOMC announcements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FdevI6L7L3N3"
      },
      "outputs": [],
      "source": [
        "def group_text_into_blocks (sentence_list: list) -> list:\n",
        "    \"\"\"\n",
        "    This function will take in the list of filtered sentences as input variable and output another list\n",
        "\n",
        "    Since the maximum filtered sentence length is 72 words, this function will attempt to group as many texts as possible\n",
        "    with this length.\n",
        "\n",
        "    The following steps will be taken:\n",
        "\n",
        "    1) Create a list of lists from each sentence\n",
        "\n",
        "        1.0) Loop through each sentence\n",
        "\n",
        "        1.1) On each sentence, split the sentence into a list of words\n",
        "\n",
        "        1.2) Append the list of words into a list\n",
        "\n",
        "    2) Create Dataset from blocks\n",
        "\n",
        "        2.0) Create two variables: dataset_variable, block_variable\n",
        "\n",
        "        2.1) Loop through each list on the list created in 1)\n",
        "\n",
        "            2.1.1) If the length of the list is lower than 72, add to the block_variable list\n",
        "\n",
        "            2.1.2) Repeat until length of current sentence + block_variable size >= 72\n",
        "\n",
        "            2.1.3) Append block_variable to dataset_variable. Note that on 2.1.2 if it exceeds 72 the current sentence \n",
        "            is not added to the current block and moved on to the next.\n",
        "\n",
        "            2.1.4) Clear the block variable \n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1)\n",
        "    text_list = []\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "        sentence_to_add = sentence.split()\n",
        "        text_list.append(sentence_to_add)\n",
        "    \n",
        "    # Step 2)\n",
        "    dataset_ = []\n",
        "    block_ = []\n",
        "\n",
        "    for sentence in text_list:\n",
        "        if len(sentence) + len(block_) <= 72:\n",
        "            for word in sentence:\n",
        "                block_.append(word)\n",
        "        else:\n",
        "            text_to_add = \" \".join(block_)\n",
        "            dataset_.append(text_to_add)\n",
        "            block_ = []\n",
        "            for word in sentence:\n",
        "                block_.append(word)\n",
        "    \n",
        "    return dataset_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv2l3wfZMTpK"
      },
      "source": [
        "### 5.3.1.5 Labeled Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmCS2QsmMnHU"
      },
      "source": [
        "Labeled Dataset Creation merges the two types of text (adversity and prosperity) into one file. The labeled dataset consists of the previously built block of texts with a label according to their period. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qn6cS0wuMboY"
      },
      "outputs": [],
      "source": [
        "def create_labeled_dataset(text_list: list, isAdversity: bool):\n",
        "    \"\"\"\n",
        "    Function that takes the texts that are already made into blocks (text_list) and returns a pandas dataframe with \n",
        "    an additional label 0 or 1 depending on the classification.\n",
        "    \"\"\"\n",
        "\n",
        "    if isAdversity:\n",
        "        label = [0 for i in range(0, len(text_list))]\n",
        "    else:\n",
        "        label = [1 for i in range(0, len(text_list))]\n",
        "    \n",
        "    dataset_dict = {\n",
        "        'text':text_list,\n",
        "        'label':label\n",
        "    }\n",
        "\n",
        "    dataset_df = pd.DataFrame(dataset_dict)\n",
        "\n",
        "    return dataset_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKPQstqrM2MD"
      },
      "source": [
        "### 5.3.1.6 Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "efx_K2iekRud"
      },
      "outputs": [],
      "source": [
        "# Loading revised vocabulary\n",
        "vocabulary = revised_vocabulary['word'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7xuCO2LyM6MX"
      },
      "outputs": [],
      "source": [
        "# Execution of all functions on the two groups of files: adversity and prosperity files\n",
        "\n",
        "# Adversity files\n",
        "adversity_df = create_dataframe(filepath=adversity_files, vocabulary=vocabulary, pos_list=pos_list, isAdversity=True)\n",
        "clean_adversity_df_as_list = clean_dataframe(adversity_df)\n",
        "adversity_text_blocks = group_text_into_blocks(clean_adversity_df_as_list)\n",
        "adversity_dataset = create_labeled_dataset(text_list=adversity_text_blocks, isAdversity=True)\n",
        "adversity_dataset.to_csv('adversity_dataset_unbalanced.csv')\n",
        "\n",
        "# Prosperity files\n",
        "prosperity_df = create_dataframe(filepath=prosperity_files, vocabulary=vocabulary, pos_list=pos_list, isAdversity=False)\n",
        "clean_prosperity_df_as_list = clean_dataframe(prosperity_df)\n",
        "prosperity_text_blocks = group_text_into_blocks(clean_prosperity_df_as_list)\n",
        "prosperity_dataset = create_labeled_dataset(text_list=prosperity_text_blocks, isAdversity=False)\n",
        "prosperity_dataset.to_csv('prosperity_dataset_unbalanced.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qetNaSO3NEFh"
      },
      "source": [
        "## 5.3.2 Data Balancing and Shuffling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vzq3b4kdNHuF"
      },
      "outputs": [],
      "source": [
        "def balance_labeled_datasets(prosperity_dataset, adversity_dataset):\n",
        "    \"\"\"\n",
        "    Function that compares the size of both datasets and performs the appropriate sampling procedure.\n",
        "\n",
        "    It takes in the dataframes of both labels: prosperity and adversity.\n",
        "\n",
        "    If one label has more data points than the other, it will be downsampled\n",
        "    \"\"\"\n",
        "    if len(prosperity_dataset) == len(adversity_dataset):\n",
        "        print(\"Both datasets are equal in size.\")\n",
        "        return\n",
        "    elif len(prosperity_dataset) > len(adversity_dataset):\n",
        "        print(\"Prosperity dataset larger than adversity dataset, it will be downsampled.\")\n",
        "        downsampled_labeled_df = prosperity_dataset.sample(len(adversity_dataset))\n",
        "        return downsampled_labeled_df, adversity_dataset\n",
        "    else:\n",
        "        print(\"adversity dataset larger than the prosperity dataset, it will be downsampled.\")\n",
        "        downsampled_labeled_df = adversity_dataset.sample(len(prosperity_dataset))\n",
        "        return downsampled_labeled_df, prosperity_dataset\n",
        "\n",
        "\n",
        "def join_and_shuffle_datasets(prosperity_dataset, adversity_dataset):\n",
        "    \"\"\"\n",
        "    Function joins the downsampled dataset with the other one and forms the final dataset.\n",
        "    \"\"\"\n",
        "    joint_dataset = pd.concat([prosperity_dataset, adversity_dataset])\n",
        "    shuffled_dataset = joint_dataset.sample(frac=1, random_state=45).reset_index()\n",
        "    shuffled_dataset.drop(columns=['Unnamed: 0', 'index'], inplace=True)\n",
        "    return shuffled_dataset\n",
        "\n",
        "\n",
        "def check_dataset_balance(balanced_dataset):\n",
        "    \"\"\"\n",
        "    Function takes in the balanced dataset, creates two sub-datasets according to the label.\n",
        "    Then, it compares the size of each sub-dataset.\n",
        "    If both sub-datasets are of equal size, it will raise no errors.\n",
        "    \"\"\"\n",
        "\n",
        "    positive_label_dataset = balanced_dataset[balanced_dataset['label']==1]\n",
        "    negative_label_dataset = balanced_dataset[balanced_dataset['label']==0]\n",
        "\n",
        "    assert len(positive_label_dataset) == len(negative_label_dataset)\n",
        "\n",
        "    balanced_dataset.to_csv('balanced_dataset.csv')\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "L2a1UHs-NQnb"
      },
      "outputs": [],
      "source": [
        "# Do not forget to check the names of the output of the dataset\n",
        "prosperity_df = pd.read_csv('prosperity_dataset_unbalanced.csv', index_col=False)\n",
        "adversity_df = pd.read_csv('adversity_dataset_unbalanced.csv', index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTNh6nMXNhdE",
        "outputId": "5dc6fd69-9a9e-4024-8764-52985adc0365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prosperity dataset larger than adversity dataset, it will be downsampled.\n"
          ]
        }
      ],
      "source": [
        "downsampled_dataset, normal_sample_dataset = balance_labeled_datasets(prosperity_dataset=prosperity_df, adversity_dataset=adversity_df)\n",
        "machine_learning_dataset = join_and_shuffle_datasets(downsampled_dataset, normal_sample_dataset)\n",
        "check_dataset_balance(machine_learning_dataset)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "1_2_Vocabulary_to_Dataset_Creation.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
